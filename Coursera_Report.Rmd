---
title: "Predicting Blight in Detroit"
author: "Stuart Barnum"
date: "5/17/2018"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#####My aim in this project is to gain insight into patterns of urban blight, using open data sets published by the city of Detroit. Blight is understood in terms of issues related to buildings: citations for offenses such as failure to maintain a building or its grounds, blight-related complaints to a city-run hotline, local crime rates, and indicatations that the building is or was likely to be demolished. A building is deemed to have become irredemably blighted if city records indicate that the building was either demolished or likely to be demolished. Given this operational definition, I trained a variety of algorithms for predicting which buildings will become irredemably blighted. Although as many many as 15 predicitors appear to be at least somewhat effective as predictors---removing any one them appears to make make the most effective models at least somewhat less predictive---two of the predictors stand out as most effective: the numer of buildings, other than the building for which the prediction is taking place, within 200 meters that have have become irredemably blighted and the number of vacant lots within 200 meters. Using a decision tree to predict on the basis these variables alone consistently produced a model with 70% predictive accuracy. The more sophisticated approaches, random forest and adaboost, made use of all 15 of the variables and produced predictive accuracy of roughly 73%.

#### General methods
After a significant aount of data cleaning (as described in the next section), one of the first tasks in this project was to define a list of buildings. This could be done in one of at least two ways. In the first, suggested as a possible method in the instructions for this assignment, buildings might be identified with clusters of incidencts such as, perhaps, recorded crime indidents and citations for blight. I elected not to used this method for a number of reasons. The first reason is the possibility that there may be clusters of incidents associated with nonbuildings such as vacant lots and parks, and the possibility that there may be buildings at which there were very few or no incidents. As it would be impossible to identify all or most of the cases of either of these types, such cases may distort the analysis. Another reason was the availability of a relatively clean set of parcel (property lot) data, which included both geographical information about the parcels and information about any buildings on the parcels. Buildings can thus be represented in terms of a subset of these parcels. One of the drawbacks of this latter approach is that some of the parcels contain, or have contained, more than one building. And although it would be possible to eliminate from the data the parcels that, according to the data, *currently* contain more than one building, it is not not possibe to determine from the available data the parcels that have, in the past, contained more than one building (for the cases in which, for example, all of the buildings within a parcel have been demolished). Although either of the two methods may provide valuable insight, I elected to use the parcel data. The analysis was done on parcels that have recently contained, or have contained (since May of 2016) at least one building. (For example, although the data may indicate that no building currently exists on a given parcel, the data may indicate that a building on that parcel was dismantled in 2017, thus indicating that a building existed on that parcel such May of 2016.)

The next step in the analysis was to assign a set of labels, irredeamably blighted or not irredeamably blighted (henceforth simply "blighted" or "not blighted"), to the buildings. A building is deemed to be or have been blighted if it (1) was demolished, as indicated in an online list of buildings that have been demolished under the Detroit Demolitions Program (see https://data.detroitmi.gov/Property-Parcels/Detroit-Demolitions/rv44-e9di), (2) is contained in a list of upcoming demolitions under this program (see https://data.detroitmi.gov/Property-Parcels/Upcoming-Demolitions/tsqq-qtet) or (3) has a demolition permit associated with it (see https://data.detroitmi.gov/Property-Parcels/Building-Permits/xw2a-a7tf, in which the demolition permits are "building permits" for which the specified type is "dismantle"). I found it useful to use all of these datasets because there were reasons to believe that any one of them would fail to list significant numbers of blighted parcels. For example, the documentation associated with the completed demolitions datset indicated that the dataset fails to include some demolitions that were completed on an emergency basis. On the other hand, there are a significant number of buildings listed in the completed demolitions dataset that, based on the demolitions permits data, appear to not have had demolition permits associated with them, thus suggesting that the demolitions permits data is also incomplete. 

I should note that one possible fault in my approach to the labeling is with the fact that we used the demolitions permits data within our operational construal of blight. After all, for example, a wealthy resident might purchase an impeccably maintained home and then demolish it (with the requisite permit) to build a larger home. Likewise in the case of other buildings demolished in order to make room for other buildings. With this issue in mind, I restricted my analyis to those areas within Detroit that that have been identified as Hardest Hit Areas (see http://d3-d3.opendata.arcgis.com/datasets/383eb730952e470389f09617b5448026_0), for which federal funding is available for the demolition program. The assumption, here, is that, in such areas, a smaller proportion of the buildings that were torn down were sound (non-blighted) buildings that were, again, torn down simply to make room for other buildings (or parking lots, etc.).

Another matter of fundamental methods concerns the manner of association of the various potential predictors, such as numbers of blight-related citations associated with a particular building, with the relevant buildings. To make the associations, I used both spatial relationships and, where both necessary and possible, parcel numbers. For example, each record of a blight-related citation includes both a parcel number (identifier of the parcel) and a pair of latitude and longitude coordinates. If the position indicated by the coordinates was within a certain parcel, than that parcel was assumed to be the parcel associated with the citation. Otherwise, if an association could be made by means of identity of parcel numbers (the parcel number for a buildings the parcel number recorded with the citation), then the parcel thus associated was deemed to be the parcel associated with the citation. Other associations were made merely by means of geometric relationships. For example, recorded crime incidents, divided between violent crimes and property crimes, were associated with parcels in virtue of spacial proximity of 200 meters. 

#### Data munging and cleaning
The project was implemented in R and, at the data munging and cleaning stages, made extensive use of the tidyverse packages for data manipulation, ggmap for investigative visual maps and for geocoding in cases of missing position coordinates, and the sf package for handling spacial information and relationships. The R sf package provides an implementation of the simple features standard, ISO 19125-1:2004, for representing real-world objects in computers. Data frames in which spatial information is thus represented become simple features data frames, and it was possible to read the shapefile-format parcels dataset (see https://data.detroitmi.gov/Property-Parcels/Parcel-Map/fxkw-udwf/data) directly into a simple features dataframe. Likewise in the case of datasets I used for the hardest hit areas of and for the council districts---these relatively small files, in the form of simple maps of of the geometries of the respective areas of the city, were read directing into the simple features format. The other datasets, containing geographic point information, were converted into simple features data frames after I had both eliminated some geographical coordinate information that was clearly incorrect (e.g. blight citations for which the coordinate information indicated a position well outside of Detroit) and, where possible, filled in the missing coordinate information by means of the ggmap `geocode` function, which accessess the Google API for geocoding. Among the data in all of the datasets I used in this project, a total of roughly 5 thousand locations were geocoded. Data for which it was impossible to obtain usable location information was discarded. The conversion into an sf data frame also required some string manipulation, of the raw coordinate information. The coordinate reference system (providing the mapping of coordinates to locations) for all of the sf data frames was 4326, which is standardly used in GPS systems. (See https://www.nceas.ucsb.edu/~frazier/RSpatialGuides/OverviewCoordinateReferenceSystems.pdf for a brief overview of coordinate reference systems.)

Another aspect of the data cleaning involved the parcels dataset. Two notable issues in this data were (1) identity of the parcel numbers among pairs of rows for which the parcel geometries were disjoint (non-overlapping) and (2) identity of the space covered by certain pairs of rows for which the parcel numbers were distinct. Although there were less than 100 pairs of either of these types, I addressed these issues as follows. For pairs of the former type (1), I made the parcel numbers (represented as strings in the data) distinct by appending unique identifiers at the end. For pairs of the latter type (2), I eliminated one of elements of each pair from the data. All of these cases (both (1) and (2)) were identified by means of the `sf` function `st_join`, by which one may implement SQL-like joins on the basis of spatial relationships. This required that the spherical representation of the latitude and longitude coordinates be projected into a plane, which, given the relatively small size of the city of Detroit in relation to Earth, provides a roughly accurate reresentation.

Another aspect of the parcels data that I investigated carefully was several thousand pairs of parcels that, according to an application of st_join, overalapped. I plotted a random sample of 100 of these pairs of parcels and found no evidence of overlap in the plots. I concluded that the apparent overlap may have been due to the projection (perhaps imperfect projection) of the spherical representation (coordinate representation system 4326 as per above) into a flat representation, and was likely not a problematic issue in my data.

After the various investigative and cleaning steps such as the obove were completed, I decided to restrict my analysis to those parcels in the Hardest Hit Areas, and so I cut out all of the parcels that were not within one of these areas (using, again, st_join). I constructed a set of labels for the remaining parcels, consisting of, for each parcel, the parcel number and the correct value with respect to blight (blighted or not blighted). I then cut out most of the rows in the labels dataset for which the value with respect to blight indicated *not blighted*, so as to create a relative ballance between positive instances (blighted) and negative instanced (not blighted). The parcels thus removed from the data were selected at random.

#### Construction of a tallies dataset
The predictive models that were eventually constructed in this project used data in the form of tallies (and other sums) from the data that had been processed as described above. All of the tallies were calculated using SQL-style joins on the basis of spatial relationships (st_join, again) and then, as in SQL, grouping and and counting the relevant results (using the R dplyr package). In the applications of st_join, there were two types of tallies. In the first, I simply looked for incidents, such as citations for failure to maintain a property, occuring within a parcel. The other type of application of st_join involved looking for incidents that were within a certain distance from a parcel. In the latter type of case, I created a "buffer" around each parcel using the sf operation st_buffer, thus expanding each each of the parcels by a certain distance, depending on what seemed reasonable, given what I was attempting to count. With the parcels thus expanded, I applied st_join, now looking for incidents (and some other entities such as vacant lots) within the expanded parcels. At the end of all of this, the tallies dataset consisted of the parcel numbers for parcels in the ballanced set of labels as desscrbed above, the value corresponding to blighted or not blighted, and 14 variables providing such tallies. As indicated in the following summary statistics, the dataset consists of 4283 rows.

```{r, message = FALSE}

library(tidyverse)
complete_tally_set <- read_rds("calculated_tallies.rds")
summary(complete_tally_set)

```

Further details about this dataset (for example, the manner in which the crime-incident data was divided into violent crimes, property crimes, and nuiscance crimes), including the code that was used in its construction from the raw data, can be found at https://stuartbarnum.github.io/Detroit-Demolitions/Detriot_Draft_3.html. In the end, because our analysis is restricted to dismantle permits and demolitions after May of 2016, the variable `later_recorded_crime_incents` was not used in our models. I should also note that, as I built-up the models, I did not treat all of the incidents in each of the datasets the same. For example, I assumed that complaints from, say, neighbors about failure to keep up a building would have a clear association with blight on the property for which the complaint was made, whereas complaints for failure of the city to fill-in potholes in a certain area may have a less clear connection with blight. Likewise, although it is difficult to make the distinction in a fully satisfactory way, I created separate variables for property crime and violent crime. And in fact, in many of my decision-tree models, violent crime had a week but positive association with blight, whereas property crime had a weak but negative assocaition with blight. 

#### Models
For the modelling, I removed 15% of the data, to be reserved for a final check on our models. I then used the `createFolds` function from the `caret` package to partition the remaining data into 10 subsets, for k-fold cross checking. I constructed a number of models as I built-up the tally set, beginging with both decision-tree models using `rpart` and logistic regression models using `glm`, on the the numbers of blight violations for each parcel and the total among of fines for blight violations for each parcel. The initial decision-tree models on these two variables achieved an accuracy of rougly 60% percent, while the logistic regression models achieved an accuracy of 58%. Both of these model-types suggested that fine totals for each parcel provided a somewhat better predictor than number of violations for each parcel. Both of the model-types were notably poor predictors in the case of positive instances (buildings that were blighted), as the following reconstruction, which includes confusion matrices for the k-fold cross-checking, indicates (actual blight corresponds to truth = 1). 

```{r}
library(rpart)

#the parcel numbers for the examples that were not reserved for final testing
training_parcelnums <- read_rds("training_parcelnums.rds")

#separate the examples to be used for the training from the fifteen percent of 
#the examples to be withheld for final testing
train <- complete_tally_set %>% filter(parcelnum %in% training_parcelnums)
#test <- blight_violation_tallies %>% filter(parcelnum %in% testing_parcelnums)
train$blighted <- as.factor(train$blighted)

#partition the training set into ten subsets, while maintaining a ballance between 
#expamples labeled as blighted and examples not so labeled
set.seed(294)
k_folds <- caret::createFolds(train$blighted)

models <- 1:10 %>% map(~ rpart(blighted ~ total_fines_by_parcel + 
                                 number_of_violations_by_parcel,
                               data = train[-k_folds[[.x]],]))

predictions <- 1:10 %>% map(~ predict(models[[.x]], newdata = train[k_folds[[.x]],], 
                                      type = "class"))

accuracies <- 1:10 %>% 
  map(~ mean(unlist(predictions[[.x]] == train[k_folds[[.x]],]$blighted)))

#summary statistics over the models
mean(unlist(accuracies))
sd(unlist(accuracies))

#confusion matrices for the models
for (index in 1:10) {
  print(table(pred = (predictions[[index]]), 
              truth = train[k_folds[[index]],]$blighted))
}

#All of the rpart models in the k-fold cross-checking contained only one split---on 
#the total amount of fines related to blight on the parcel, as in the following.
models[[3]]

```

As more variables were added to the tally dataset, I also applied random forest (using the `randomForest` package) and, finally, adaboost (using my own implementation in R), and support vector machines (using the `e1071` package). After the tally datset was complete, I attempted to tune the models by changing the values of some of the parameters, such as the complexity parameter associated with the rpart function. In the end, most of the improvement in accuracy as I worked on this project was with the introduction of variables concerning blight at nearby parcels, espeically in the case of the number of nearby vacant lots (other than the parcel for which prediction is taking place) and the number of nearby blighted parcels (other than the parcel for which prediction is taking place). The decision-tree models achieved an average accuracy, accross the the ten models constructed in the k-fold cross checking of 71% with the with the complexity parameter set at 0.003 (reduced from the default value of 0.01). Similar calculations of average accuracies for random forest and adaboost models yielded 0.73. The accuracy of 0.73 in the adaboost was achieved with the trees ( the "weak" classifiers) in the form of decision stumps---trees that contained at most one split, and in some cases no splits at all. Adaboost over trees containing more than one split resulted in accuracies at least somewhat less than this.

I should emphasize that the counts of nearby buildings that were blighted did not include the building for which we were predicting. (Otherwise, the analysis would be circular.) Neverthess, I tried running some of the approaches to model-building without using the variable for nearby blight. For the decision-tree models, this resulted in an average accuracy, over the 10 models contructing during k-fold cross checking, of 0.70. 

#### Final thoughts and analysis

