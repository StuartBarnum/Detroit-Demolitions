---
title: "Detroit_models_from_tallies"
author: "Stuart Barnum"
date: "5/18/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Packages used repeatedly:

```{r}
library(tidyverse)
library(rpart)
library(randomForest)

```


Data and other information from `Detroit_Draft_3.Rmd`:

```{r}

complete_tally_set <- read_rds("calculated_tallies.rds")

complete_formula <- blighted ~ total_fines_by_parcel + number_of_violations_by_parcel + 
  improve_issues_tallies + earlier_property_crime_incidents + 
  earlier_violent_crime_incidents + total_acre + council_di + frontage + num_vacant_parcels +
  num_nearby_blighted_parcels + num_violations_nearby_parcels + earlier_nuiscance_offences

training_parcelnums <- read_rds("training_parcelnums.rds")

```

We partition the dataset as per k-fold cross validation, and train rpart models on the respective portions

```{r}

#separate the examples to be used for the training from the twenty percent of the examples to be withheld for final testing (remembering that we will do 10-fold cross validation over the training set)
train <- complete_tally_set %>% filter(parcelnum %in% training_parcelnums)
#test <- blight_violation_tallies %>% filter(parcelnum %in% testing_parcelnums)
train$blighted <- as.factor(train$blighted)

#partition the training set into ten subsets, while maintaining a ballance between expamples labeled as blighted and examples not so labeled
set.seed(451)
k_folds <- caret::createFolds(train$blighted)

models <- 1:10 %>% map(~ rpart(complete_formula, data = train[-k_folds[[.x]],],
                               method = "class", control = rpart.control(cp = 0.003)))

predictions <- 1:10 %>% map(~ predict(models[[.x]], newdata = train[k_folds[[.x]],], type = "class"))
predictions <- 1:10 %>% map(~as.numeric(predictions[[.x]]) - 1)

accuracies <- 1:10 %>% 
  map(~ mean(as.numeric(! predictions[[.x]] < 0.5) == train[k_folds[[.x]],]$blighted))

#summary statistics over the models
mean(as.numeric(accuracies))
sd(as.numeric(accuracies))

#confusion matrices for the models
for (index in 1:10) {
  print(table(pred = (predictions[[index]] > 0.5), truth = train[k_folds[[index]],]$blighted))
}

models[[3]]

```

Now try glm

```{r}

models <- 1:10 %>% map(~ glm(complete_formula, data = train[-k_folds[[.x]],], family = "binomial"))

predictions <- 1:10 %>% map(~ predict.glm(models[[.x]], newdata = train[k_folds[[.x]],],
                                      type = "response"))
accuracies <- 1:10 %>% 
  map(~ mean(as.numeric(! predictions[[.x]] < 0.5) == train[k_folds[[.x]],]$blighted))


#summary statistics over the models
mean(as.numeric(accuracies))
sd(as.numeric(accuracies))

#confusion matrices for the models
for (index in 1:10) {
  print(table(pred = (predictions[[index]] > 0.5), truth = train[k_folds[[index]],]$blighted))
}

models[[9]]

```

Now try support vector machines, without normalizing the variables.

```{r}
library(e1071)

#separate the examples to be used for the training from the twenty percent of the examples to be withheld for final testing (remembering that we will do 10-fold cross validation over the training set)
train <- complete_tally_set %>% filter(parcelnum %in% training_parcelnums)
#test <- blight_violation_tallies %>% filter(parcelnum %in% testing_parcelnums)
train$blighted <- as.factor(train$blighted)

#partition the training set into ten subsets, while maintaining a ballance between expamples labeled as blighted and examples not so labeled
set.seed(451)
k_folds <- caret::createFolds(train$blighted)

models <- 1:10 %>% map(~ svm(complete_formula, data = train[-k_folds[[.x]],]))

predictions <- 1:10 %>% map(~ predict(models[[.x]], newdata = train[k_folds[[.x]],]))

accuracies <- 1:10 %>% 
  map(~ mean(unlist(predictions[[.x]] == train[k_folds[[.x]],]$blighted)))

#summary statistics over the models
mean(unlist(accuracies))
sd(unlist(accuracies))

#confusion matrices for the models
for (index in 1:10) {
  print(table(pred = (predictions[[index]]), 
              truth = train[k_folds[[index]],]$blighted))
}

#All of the rpart models in the k-fold cross-checking contained only one split---on 
#the total amount of fines related to blight on the parcel, as in the following.
models[[3]]
```

Now try support vector machines, after normalizing the numerical variables

```{r}

train$blighted <- as.factor(train$blighted)

#cut out the non-numeric columns, scale the numeric columns, and then put the non-numerica columns back in
train_svm <- train %>% select(-parcelnum, -blighted, -council_di) %>% scale() %>% as.tibble() %>% 
  mutate(parcelnum = train$parcelnum, blighted = train$blighted, council_di = train$council_di)

set.seed(451)
k_folds <- caret::createFolds(train_svm$blighted)

models <- 1:10 %>% map(~ svm(complete_formula, data = train_svm[-k_folds[[.x]],]))

predictions <- 1:10 %>% map(~ predict(models[[.x]], newdata = train_svm[k_folds[[.x]],]))

accuracies <- 1:10 %>% 
  map(~ mean(unlist(predictions[[.x]] == train_svm[k_folds[[.x]],]$blighted)))

#summary statistics over the models
mean(unlist(accuracies))
sd(unlist(accuracies))

#confusion matrices for the models
for (index in 1:10) {
  print(table(pred = (predictions[[index]]), 
              truth = train_svm[k_folds[[index]],]$blighted))
}

#All of the rpart models in the k-fold cross-checking contained only one split---on 
#the total amount of fines related to blight on the parcel, as in the following.
models[[3]]



```



Now try random forest:

```{r}
library(randomForest)

models <- 1:10 %>% map(~ randomForest(complete_formula, 
                                      data = train[-k_folds[[.x]],], ntree = 2000))

predictions <- 1:10 %>% map(~ predict(models[[.x]], newdata = train[k_folds[[.x]],], type = "class"))
predictions <- 1:10 %>% map(~as.numeric(predictions[[.x]]) - 1)

accuracies <- 1:10 %>% 
  map(~ mean(as.numeric(! predictions[[.x]] < 0.5) == train[k_folds[[.x]],]$blighted))

#summary statistics over the models
mean(as.numeric(accuracies))
sd(as.numeric(accuracies))

#confusion matrices for the models
for (index in 1:10) {
  print(table(pred = (predictions[[index]] > 0.5), truth = train[k_folds[[index]],]$blighted))
}

models[[3]]

```

Now try adaboost. The first step is to define an adaboost function, in terms of rpart.

```{r}

#returns a prediction on the "test" dataset
demolition_adaboost <- function(ada_train, ada_test, formula, iterations, tree_depth, min_cp) {
  
  num_rows <- nrow(ada_train)
  
  #initialize the weights to be applied to each example in the training set
  ada_train$wts <- 1/num_rows         
  
  models <- list() #initializing the list of rpart models
  pred_weights <- list() #initiallizing a sum to be taken over the iterations in the for loop
  adjustment_factors <- list() #initializing a list of the factors used to adjust the example weights
  pred_list <- list()  #predictions on the test dataset from EACH tree (NOT the sum)
  for (index in 1:iterations) {
    
    #In this implementation of Adaboost, the test set is sampled with replacement, and then the 
    #weights (as prescribed in the Adaboost meta-algorithm) are used to determine the relative import
    #of the respective datapoints in an rpart implementation on the sample. I have also experimented with 
    #using the weights to determine the relative probabilities for selection in the sample. However, the
    #performance (accuracy in cross-validation) was rather disapointing, with many of the weights rather                    #quickly converging  to zero. 
    
    ada_resample <- sample_n(ada_train, num_rows, replace = TRUE)
    
    #the weights for the rpart model
    wts <- ada_resample$wts
    
    model <- rpart::rpart(formula, data = ada_resample, 
                   method = "class", weights = wts,
                   control = rpart.control(maxdepth = tree_depth,
                                           cp = min_cp))
      
    #predict over the entire training set
    prediction <- predict(model, newdata = ada_train, type = "class")
    ada_train$Prediction <- prediction
      
    #The predictions on the proper test set are calculated here:
    pred <- predict(model, newdata = ada_test, type="class")
    ada_test$Prediction <- pred
    
    ####ada_test <- arrange(ada_test, parcelnum)
    pred_list[[index]] <- as.numeric(ada_test$Prediction) - 1  #stores prediction for the test set
   
    #We now handle the calculation of the adjustment factor and its application to the example weights
   
     wrong_cases <- ada_train[ada_train$blighted != ada_train$Prediction,]
    sum_weights_misclassified <- sum(wrong_cases$wts)  #epsilon
    adjustment_factor <- 
      sqrt(sum_weights_misclassified / (1 - sum_weights_misclassified))  #beta
    correct_cases <- ada_train[ada_train$blighted == ada_train$Prediction,]
    
    #apply the adjustment factors to the weights
    ada_train <- transform(ada_train,
                      wts = ifelse(parcelnum %in% correct_cases$parcelnum,
                                        wts * adjustment_factor, wts / adjustment_factor))
    #renormalize the weights
    ada_train <- transform(ada_train,
      wts = wts/sum(wts))
    
    #save the weight on the model(s) in this iteration, the model(s), and the adjustment factor
    #in a list (for both calculating the adaboost prediction and for examining after running the
    #program)
    pred_weights[[index]] <- log((1-sum_weights_misclassified)/sum_weights_misclassified)
    models[[index]] <- model
    adjustment_factors[[index]] <- adjustment_factor
    }
  
  #Apply the weighted models to the test data, to derive our predictions
  sum_weighted_predictions <- 0 #initialize the weighted sum of the predictions 
 
  #initialize a list in which the i'th element is the the adaboost prediction for i iterations
  prediction_list <- list() 
  accuracy_list <- list()
  #confusion_matrix_list <- list()
  for (index in 1:iterations) {
    
    pred <- pred_list[[index]] 
    sum_weighted_predictions <- 
      sum_weighted_predictions + (pred - 0.5)*pred_weights[[index]]
    prediction_list[[index]] <- as.numeric(sum_weighted_predictions > 0)
    accuracy_list[[index]] <- mean(as.numeric(prediction_list[[index]] == ada_test$blighted))
    }
  
  predictions_df <- tibble(parcelnum = ada_test$parcelnum, truth = ada_test$blighted)
  for (index in 1:iterations) {
    predictions_df[[index + 2]] <- prediction_list[[index]]
  }
  
  #return a comprehensive set of information
  return(list(predictions = predictions_df, 
              accuracies = accuracy_list, 
              rpart_models = models, 
              prediction_weights = pred_weights,
              example_weights = ada_train$wts,
              adjustment_factors = adjustment_factors,
              predicted_parcels = ada_test$parcelnum))
  }

```

We now apply the function for training and k-fold cross validation, using a parallel loop for the cross-validation. My various implementations of this approach to the training data has suggested that the adaboost model is most-predictive at between 800 and 1200 boosting iterations. However, for any number of iterations, the accuracy of the models varies. I thus used the average of the predictions with the range from 800 to 1200, rounded to 0 or 1.
```{r}

ada_iterations <- 3000
tree_depth <- 1
minimum_cp <- 0.006

library(doParallel)
library(foreach)
cl <- makePSOCKcluster(6)
registerDoParallel(cl)
model_info_list <- 
  foreach(index = 1:10,  
          .packages = c("tidyverse", "rpart")) %dopar% { 
            
  cross_train <- train[-k_folds[[index]],]
  cross_test <- train[k_folds[[index]],]
  
  #the information to be added to model_info_list, at this iteration of parallel loop:
  
  prediction_and_info <- 
    demolition_adaboost(cross_train, cross_test, complete_formula, ada_iterations, 
                        tree_depth, minimum_cp)
  }
stopCluster(cl)
rm(cl)

combined_predictions_df <- model_info_list[[1]]$predictions
for (index in 2:10) {
  combined_predictions_df <- rbind(combined_predictions_df, 
                                   model_info_list[[index]]$predictions)
}

predictions <- combined_predictions_df %>% select(800:1200) %>%
  rowMeans()

predictions <- round(predictions)

mean(as.numeric(predictions == combined_predictions_df$truth))
```

```{r}

rm(model_info_list)
gc()

```


Consider the Pearson correlations coefficients (point biserials):

```{r}

correlations <- complete_tally_set %>% 
  select(-parcelnum) %>%
  mutate(council_di = as.numeric(council_di)) %>%
  cor %>%
  as.data.frame() %>%
  select(blighted)

correlations

```

Cut out the two variables with the weakest correlation:

```{r}

#the complete formula, again
formula <- blighted ~ number_of_violations_by_parcel + total_fines_by_parcel + earlier_property_crime_incidents + earlier_violent_crime_incidents + total_acre + council_di + frontage + num_vacant_parcels + num_nearby_blighted_parcels + num_violations_nearby_parcels + sum_fines_nearby_parcels +
earlier_nuiscance_offences

#the formula with some weak variables removed
formula <- blighted ~ number_of_violations_by_parcel + total_fines_by_parcel + improve_issues_tallies + earlier_property_crime_incidents + earlier_violent_crime_incidents + total_acre + council_di + frontage + num_vacant_parcels + num_nearby_blighted_parcels + num_violations_nearby_parcels + earlier_nuiscance_offences

models <- 1:10 %>% map(~ rpart(formula, data = train[-k_folds[[.x]],], 
                               control = rpart.control(cp = 0.005)))

predictions <- 1:10 %>% map(~ predict(models[[.x]], newdata = train[k_folds[[.x]],], type = "class"))
predictions <- 1:10 %>% map(~as.numeric(predictions[[.x]]) - 1)

accuracies <- 1:10 %>% 
  map(~ mean(as.numeric(! predictions[[.x]] < 0.5) == train[k_folds[[.x]],]$blighted))

#summary statistics over the models
mean(as.numeric(accuracies))
sd(as.numeric(accuracies))

#confusion matrices for the models
for (index in 1:10) {
  print(table(pred = (predictions[[index]] > 0.5), truth = train[k_folds[[index]],]$blighted))
}

models[[2]]


```

```{r} 
models <- 1:10 %>% map(~ glm(formula, data = train[-k_folds[[.x]],], family = "binomial"))

predictions <- 1:10 %>% map(~ predict.glm(models[[.x]], newdata = train[k_folds[[.x]],],
                                      type = "response"))
accuracies <- 1:10 %>% 
  map(~ mean(as.numeric(! predictions[[.x]] < 0.5) == train[k_folds[[.x]],]$blighted))


#summary statistics over the models
mean(as.numeric(accuracies))
sd(as.numeric(accuracies))

#confusion matrices for the models
for (index in 1:10) {
  print(table(pred = (predictions[[index]] > 0.5), truth = train[k_folds[[index]],]$blighted))
}

models[[2]]

```


1) root 3276 1602 0 (0.5109890 0.4890110)  
  2) num_nearby_blighted_parcels< 1.5 1540  462 0 (0.7000000 0.3000000)  
    4) num_vacant_parcels< 50.5 1289  309 0 (0.7602793 0.2397207) *
    5) num_vacant_parcels>=50.5 251   98 1 (0.3904382 0.6095618) *
  3) num_nearby_blighted_parcels>=1.5 1736  596 1 (0.3433180 0.6566820) *
  

Partition the dataset according to whether `num_nearby_blighted_parcels` < 1.5 and consider the Pearson correlation coefficient (point biserials) within each of the elements of the partition:
  
```{r}

low_num_blighted_parcels <- train %>% filter(num_nearby_blighted_parcels < 1.5)

correlations <- low_num_blighted_parcels %>% 
  select(-parcelnum) %>%
  mutate(council_di = as.numeric(council_di), 
         blighted = as.numeric(blighted)) %>%
  cor %>%
  as.data.frame() %>%
  select(blighted)
correlations

higher_num_blighted_parcels <- train %>% filter(num_nearby_blighted_parcels >= 1.5)

correlations <- higher_num_blighted_parcels %>% 
  select(-parcelnum) %>%
  mutate(council_di = as.numeric(council_di),
         blighted = as.numeric(blighted)) %>%
  cor %>%
  as.data.frame() %>%
  select(blighted)
correlations

```

We try partitioning the dataset according one of the more predictive variables, then running random forest on each of the two subsets.

```{r}

formula <- blighted ~ number_of_violations_by_parcel + total_fines_by_parcel + earlier_property_crime_incidents + earlier_violent_crime_incidents + total_acre + council_di + frontage + num_vacant_parcels + num_nearby_blighted_parcels + num_violations_nearby_parcels + sum_fines_nearby_parcels +
earlier_nuiscance_offences


#k-fold models for parcels with a low number of nearby blighted parcels
models_1 <- 1:10 %>% map(~ randomForest(formula, data = train[-k_folds[[.x]],] %>%
                                        filter(num_nearby_blighted_parcels < 1.5)))

#k-fold models for parcels with a high number of nearby blighted parcels
models_2 <- 1:10 %>% map(~ randomForest(formula, data = train[-k_folds[[.x]],] %>%
                                        filter(num_nearby_blighted_parcels >= 1.5)))

predictions_1 <- 1:10 %>% map(~ predict(models_1[[.x]], newdata = train[k_folds[[.x]],] %>%
                                          filter(num_nearby_blighted_parcels < 1.5), 
                                        type = "class"))

predictions_1 <- 1:10 %>% map(~as.numeric(predictions_1[[.x]]) - 1)

predictions_2 <- 1:10 %>% map(~ predict(models_2[[.x]], newdata = train[k_folds[[.x]],] %>%
                                          filter(num_nearby_blighted_parcels >= 1.5), 
                                        type = "class"))

predictions_2 <- 1:10 %>% map(~as.numeric(predictions_2[[.x]]) - 1)

accuracies_1 <- 1:10 %>% 
  map(~ as.numeric(predictions_1[[.x]] == (train[k_folds[[.x]],] %>%
                                          filter(num_nearby_blighted_parcels < 1.5))$blighted))

accuracies_2 <- 1:10 %>% 
  map(~ as.numeric(as.numeric(! predictions_2[[.x]] < 0.5) == (train[k_folds[[.x]],] %>%
                                          filter(num_nearby_blighted_parcels >= 1.5))$blighted))

#summary statistics over the models
mean(unlist(accuracies_1))
mean(unlist(accuracies_2))

#grand mean
mean(c(unlist(accuracies_1), unlist(accuracies_2)))

```

Consider the following decision tree, which forms the rpart model when the complexity parameter is set relatively high.

1) root 3276 1602 0 (0.5109890 0.4890110)  
  2) num_nearby_blighted_parcels< 1.5 1540  462 0 (0.7000000 0.3000000)  
    4) num_vacant_parcels< 50.5 1289  309 0 (0.7602793 0.2397207) *
    5) num_vacant_parcels>=50.5 251   98 1 (0.3904382 0.6095618) *
  3) num_nearby_blighted_parcels>=1.5 1736  596 1 (0.3433180 0.6566820) *

We partition our data according to the end-nodes in our decision tree, than then train a random forest model on each of the three subsets specified by the end nodes.

```{r}

library(rlang)

condition_1 <- expr(num_nearby_blighted_parcels < 1.5 & num_vacant_parcels < 50.5)

condition_2 <- expr(num_nearby_blighted_parcels < 1.5 & num_vacant_parcels >= 50.5)

condition_3 <- expr(num_nearby_blighted_parcels >= 1.5)

cat("Case 1\n")
condition_1

correlations <- train %>% filter(!!condition_1) %>% 
  select(-parcelnum) %>%
  mutate(council_di = as.numeric(council_di), 
         blighted = as.numeric(blighted)) %>%
  cor %>%
  as.data.frame() %>%
  select(blighted)
correlations

cat("\nCase 2\n")
condition_2

correlations <- train %>% filter(!!condition_2) %>% 
  select(-parcelnum) %>%
  mutate(council_di = as.numeric(council_di), 
         blighted = as.numeric(blighted)) %>%
  cor %>%
  as.data.frame() %>%
  select(blighted)
correlations

cat("\nCase 3\n")
condition_3

correlations <- train %>% filter(!!condition_3) %>% 
  select(-parcelnum) %>%
  mutate(council_di = as.numeric(council_di), 
         blighted = as.numeric(blighted)) %>%
  cor %>%
  as.data.frame() %>%
  select(blighted)
correlations

```

```{r}

formula_1 <- blighted ~ number_of_violations_by_parcel + total_fines_by_parcel + improve_issues_tallies +
  earlier_property_crime_incidents + earlier_violent_crime_incidents + council_di +
  num_violations_nearby_parcels + earlier_nuiscance_offences

formula_2 <- blighted ~ total_fines_by_parcel + number_of_violations_by_parcel + 
  improve_issues_tallies + earlier_property_crime_incidents + 
  earlier_violent_crime_incidents + council_di + frontage + num_vacant_parcels +
  num_nearby_blighted_parcels + num_violations_nearby_parcels

formula_3 <- blighted ~ number_of_violations_by_parcel + total_fines_by_parcel + improve_issues_tallies +
  earlier_property_crime_incidents + earlier_violent_crime_incidents + total_acre + council_di + frontage +
  num_vacant_parcels + num_violations_nearby_parcels + sum_fines_nearby_parcels

#formula_1 <- formula
#formula_2 <- formula
#formula_3 <- formula

models_1 <- 1:10 %>% map(~ randomForest(formula_1, data = train[-k_folds[[.x]],] %>%
                                        filter(!!condition_1), ntree = 2000))

models_2 <- 1:10 %>% map(~ randomForest(formula_2, data = train[-k_folds[[.x]],] %>%
                                        filter(!!condition_2), ntree = 2000))

models_3 <- 1:10 %>% map(~ randomForest(formula_3, data = train[-k_folds[[.x]],] %>%
                                        filter(!!condition_3), ntree = 2000))

predictions_1 <- 1:10 %>% map(~ predict(models_1[[.x]], newdata = train[k_folds[[.x]],] %>%
                                          filter(!!condition_1), 
                                        type = "class"))

predictions_2 <- 1:10 %>% map(~ predict(models_2[[.x]], newdata = train[k_folds[[.x]],] %>%
                                          filter(!!condition_2), 
                                        type = "class"))

predictions_3 <- 1:10 %>% map(~ predict(models_3[[.x]], newdata = train[k_folds[[.x]],] %>%
                                          filter(!!condition_3), 
                                        type = "class"))

truth_1 <- 1:10 %>% map(~ (train[k_folds[[.x]],] %>% filter(!!condition_1))$blighted)

truth_2 <- 1:10 %>% map(~ (train[k_folds[[.x]],] %>% filter(!!condition_2))$blighted)

truth_3 <- 1:10 %>% map(~ (train[k_folds[[.x]],] %>% filter(!!condition_3))$blighted)

accuracies_1 <- 1:10 %>% map(~ (as.numeric(predictions_1[[.x]] == truth_1[[.x]])))

accuracies_2 <- 1:10 %>% map(~ (as.numeric(predictions_2[[.x]] == truth_2[[.x]])))

accuracies_3 <- 1:10 %>% map(~ (as.numeric(predictions_3[[.x]] == truth_3[[.x]])))

mean(unlist(accuracies_1))
mean(unlist(accuracies_2))
mean(unlist(accuracies_3))

#grand mean
mean(c(unlist(accuracies_1), unlist(accuracies_2), unlist(accuracies_3)))
cat("\n")

#grand confusion matrix
table(truth = c(unlist(truth_1), unlist(truth_2), unlist(truth_3)) - 1,
      pred = c(unlist(predictions_1), unlist(predictions_2), unlist(predictions_3)) - 1)



```

Having trained a model on the complete training set and then testing it on the 15% of the data that was withheld for final testing, I achieved an accuracy of 75%. As I was somewhat skeptical as to whether this figure would be typical, I tried tried k-fold cross testing over the entire balanced dataset.

```{r}

#ensure that the outcome variable is of type claass
complete_tally_set$blighted <- as.factor(complete_tally_set$blighted)

#set.seed(451)
full_k_folds <- caret::createFolds(complete_tally_set$blighted)
#(with the default of 10 folds)

ada_iterations <- 2000
tree_depth <- 1
minimum_cp <- 0.006

library(doParallel)
library(foreach)
cl <- makePSOCKcluster(6)
registerDoParallel(cl)
model_info_list <- 
  foreach(index = 1:10,  
          .packages = c("tidyverse", "rpart")) %dopar% { 
            
  cross_train <- complete_tally_set[-full_k_folds[[index]],]
  cross_test <- complete_tally_set[full_k_folds[[index]],]
  
  #the information to be added to model_info_list, at this iteration of parallel loop:
  
  prediction_and_info <- 
    demolition_adaboost(cross_train, cross_test, complete_formula, ada_iterations, 
                        tree_depth, minimum_cp)
  }
stopCluster(cl)
rm(cl)

#max(accuracy_averages)
#which.max(accuracy_averages)

#names(accuracy_averages) <- 1:length(accuracy_averages)

#3000
#accuracy_averages[1:1000]
#389-390
#486-495
#772-793

#max(accuracy_averages)
#.72613

#rm(model_info_list)
#gc()

combined_predictions_df <- model_info_list[[1]]$predictions
for (index in 2:10) {
  combined_predictions_df <- rbind(combined_predictions_df, 
                                   model_info_list[[index]]$predictions)
}

#for (index in 3:ncol(combined_predictions_df)) {
#  print(c(index - 2, mean(combined_predictions_df$truth == combined_predictions_df[[index]])))
#}

predictions <- combined_predictions_df %>% select(900:1100) %>%
  rowMeans()

predictions <- round(predictions)

mean(as.numeric(predictions == combined_predictions_df$truth))

```

Remove the information returned by the parallel loop, which includes each of the rpart models that were generated as part of adaboost.

```{r}

rm(model_info_list)
gc()

```

Now look at random forest over the complete balanced set.

```{r}

#ensure that the outcome variable is of type claass
complete_tally_set$blighted <- as.factor(complete_tally_set$blighted)

#set.seed(555)
full_k_folds <- caret::createFolds(complete_tally_set$blighted)
#(with the default of 10 folds)

library(randomForest)

library(doParallel)
library(foreach)
cl <- makePSOCKcluster(6)
registerDoParallel(cl)
randomForest_models <- 
  foreach(index = 1:10,  
          .packages = c("tidyverse", "randomForest")) %dopar% { 

  randomForest(formula, 
              complete_tally_set[-full_k_folds[[index]],], 
              ntree = 1000)  
            
  }
stopCluster(cl)
rm(cl)            
      
predictions <- 1:10 %>% map(~ predict(randomForest_models[[.x]], 
                                      newdata = complete_tally_set[full_k_folds[[.x]],], 
                                      type = "class"))

#predictions <- 1:10 %>% map(~as.numeric(predictions[[.x]]) - 1)

accuracies <- 1:10 %>% 
  map(~ mean(predictions[[.x]] == complete_tally_set[full_k_folds[[.x]],]$blighted))

#mean accuracy for each fold
accuracies

#mean accuracy and standard deviation over the folds
mean(unlist(accuracies))
sd(unlist(accuracies))

#confusion matrices for the models
for(index in 1:10) {
print(table(pred = predictions[[index]], 
      truth = complete_tally_set[full_k_folds[[index]],]$blighted))
}

randomForest_models[[4]]

```

