---
title: "Predicting Blight in Detroit"
author: "Stuart Barnum"
date: "5/20/2018"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#####My aim in this project is to gain insight into patterns of urban blight, using open data sets published by the city of Detroit. Blight is understood in terms of issues related to buildings: citations for offenses such as failure to maintain a building or its grounds, blight-related complaints to a city-run hotline, local crime rates, and indicatations that the building is or was likely to be demolished. A building is deemed to have become irredemably blighted if city records indicate that the building was either demolished or likely to be demolished. Given this definition, I trained a variety of algorithms for predicting which buildings will become irredemably blighted. Although as many many as 13 predicitors appear to be at least somewhat effective as predictors---removing any one them appears to make make the most effective models at least somewhat less predictive---two of the predictors stand out as most effective: the number of buildings, other than the building for which the prediction is taking place, within 200 meters that became irredemably blighted before the time over which I attempt to make predictions, and the number of vacant lots within 200 meters. Using a decision tree to predict on the basis these variables alone consistently produced a model with 70% predictive accuracy. Two of the more sophisticated approaches, random forest and adaboost, made use of all 13 of the variables and yielded a predictive accuracy of roughly 73%.

#### General methods
After a significant amount of data cleaning (as described in the next section), one of the first tasks in this project was to define a list of buildings. This could be done in one of at least two ways. In the first, suggested as a possible method in the instructions for this assignment, buildings might be identified with clusters of incidents such as, perhaps, recorded crime incidents and citations for blight. I elected not to used this method for a number of reasons. The first reason is the possibility that there may be clusters of incidents associated with non-buildings such as vacant lots and parks, and the possibility that there may be buildings at which there were very few or no incidents. As it would be impossible to identify all or most of the cases of either of these types, such cases may distort the analysis. Another reason was the availability of a relatively clean set of parcel (property lot) data, which included both geographical information about the parcels and information about any buildings on the parcels. Buildings can thus be represented in terms of a subset of these parcels. One of the drawbacks of this latter approach is that some of the parcels contain, or have contained, more than one building. And although it would be possible to eliminate from the data the parcels that, according to the data, *currently* contain more than one building, it is not possible to determine from the available data the parcels that have, in the past, contained more than one building (for the cases in which, for example, all of the buildings within a parcel have been demolished). Although either of the two methods may provide valuable insight, I elected to use the parcel data. The analysis was done on parcels that have, since May of 2016, contained at least one building. For example, although the data may indicate that no building currently exists on a given parcel, the data may indicate that a building on that parcel was dismantled in 2017, thus indicating that a building existed on that parcel.

The next step in the analysis was to assign a set of labels, irredeemably blighted or not irredeemably blighted (henceforth simply "blighted" or "not blighted"), to the buildings. A building is deemed to be or have been blighted if it (1) was demolished, as indicated in an online list of buildings that have been demolished under the Detroit Demolitions Program (see https://data.detroitmi.gov/Property-Parcels/Detroit-Demolitions/rv44-e9di), (2) is contained in a list of upcoming demolitions under this program (see https://data.detroitmi.gov/Property-Parcels/Upcoming-Demolitions/tsqq-qtet) or (3) has a demolition permit associated with it (see https://data.detroitmi.gov/Property-Parcels/Building-Permits/xw2a-a7tf, in which the demolition permits are "building permits" for which the specified type is "dismantle"). I found it useful to use all of these datasets because there were reasons to believe that any one of them would fail to list significant numbers of blighted parcels. For example, the documentation associated with the completed demolitions dataset indicated that the dataset fails to include some demolitions that were completed on an emergency basis. On the other hand, there are a significant number of buildings listed in the completed demolitions dataset that, based on the demolitions permits data, appear to not have had demolition permits associated with them, thus suggesting that the demolitions permits data is also incomplete. 

I should note that one possible fault in my approach to the labeling is with the fact that we used the demolitions permits data within our operational construal of blight. After all, for example, a wealthy resident might purchase an impeccably maintained home and then demolish it (with the requisite permit) to build a larger home. Likewise in the case of other buildings demolished in order to make room for other buildings. With this issue in mind, I restricted my analysis to those areas within Detroit that that have been identified as Hardest Hit Areas (see http://d3-d3.opendata.arcgis.com/datasets/383eb730952e470389f09617b5448026_0), for which federal funding is available for the demolition program. The assumption, here, is that, in such areas, a smaller proportion of the buildings that were torn down were sound (not genuinely blighted) buildings that were, again, torn down simply to make room for other buildings (or parking lots, etc.).

Another matter of fundamental methods concerns the manner of association of the various potential predictors, such as numbers of blight-related citations associated with a particular building, with the relevant buildings. To make the associations, I used both spatial relationships and, where both necessary and possible, parcel numbers. For example, each record of a blight-related citation includes both a parcel number (identifier of the parcel) and a pair of latitude and longitude coordinates. If the position indicated by the coordinates was within a certain parcel, than that parcel was assumed to be the parcel associated with the citation. Otherwise, if an association could be made by means of identity of parcel numbers (the parcel number for the building and the parcel number recorded with the citation), then the parcel thus associated was deemed to be the parcel associated with the citation. Other associations were made merely by means of geometric relationships (with no consideration of parcel numbers). For example, recorded crime incidents, divided between violent crimes, property crimes, and nuisance crimes, were associated with parcels in virtue of spacial proximity of 200 meters. 

#### Data munging and cleaning
The project was implemented in R and, at the data munging and cleaning stages, made extensive use of the `tidyverse` packages for data manipulation, `ggmap` for investigative visual maps and for geocoding in cases of missing position coordinates, and the `sf` package for handling spacial information and relationships. The R `sf` package provides an implementation of the simple features standard, ISO 19125-1:2004, for representing real-world objects in computers. Data frames in which spatial information is thus represented become simple features data frames, and it was possible to read the shapefile-format parcels dataset (see https://data.detroitmi.gov/Property-Parcels/Parcel-Map/fxkw-udwf/data) directly into a simple features dataframe. Likewise in the case of datasets I used for the hardest hit areas and for the council districts---these relatively small files, in the form of simple maps of the geometries of the respective areas of the city, were read directly into the simple features format. The other datasets, containing geographic point information, were converted into simple features data frames after I had both eliminated some geographical coordinate information that was clearly incorrect (e.g. blight citations for which the coordinate information indicated a position well outside of Detroit) and, where possible, filled in the missing coordinate information by means of the ggmap `geocode` function, which accesses the Google API for geocoding. Among the data in all of the datasets I used in this project, a total of roughly 5 thousand locations were geocoded. Data for which it was impossible to obtain usable location information was discarded. The conversion into an sf data frame also required some string manipulation, of the raw coordinate information. The coordinate reference system (providing the mapping of coordinates to locations) for all of the sf data frames was 4326, which is standardly used in GPS systems. (See https://www.nceas.ucsb.edu/~frazier/RSpatialGuides/OverviewCoordinateReferenceSystems.pdf for a brief overview of coordinate reference systems.)

Another aspect of the data cleaning involved the parcels dataset. Two notable issues in this data were (1) identity of the parcel numbers among pairs of rows for which the parcel geometries were disjoint (non-overlapping) and (2) identity of the space covered by certain pairs of rows for which the parcel numbers were distinct. Although there were less than 100 pairs of either of these types, I addressed these issues as follows. For pairs of the former type (1), I made the parcel numbers (represented as strings in the data) distinct by appending unique identifiers at the end. For pairs of the latter type (2), I eliminated one of elements of each pair from the data. All of these cases (both (1) and (2)) were identified by means of the `sf` function `st_join`, by which one may implement SQL-like joins on the basis of spatial relationships. This required that the spherical representation of the latitude and longitude coordinates be projected into a plane, which, given the relatively small size of the city of Detroit in relation to Earth, provides a roughly accurate representation.

Another aspect of the parcels data that I investigated carefully was several thousand pairs of parcels that, according to an application of st_join, overlapped. I plotted a random sample of 100 of these pairs of parcels and found no evidence of overlap in the plots. I concluded that the apparent overlap may have been due to the projection (perhaps imperfect projection) of the spherical representation (coordinate representation system 4326 as per above) into a flat representation, and was likely not a problematic issue in my data.

After the various investigative and cleaning steps such as the above were completed, I decided to restrict my analysis to those parcels in the Hardest Hit Areas, and so I removed from the analysis all of the parcels that were not within one of these areas (using, again, `st_join`). I constructed a set of labels for the remaining parcels, consisting of, for each parcel, the parcel number and the correct value with respect to blight (blighted or not blighted). I then cut out most of the rows in the labels dataset for which the value with respect to blight indicated *not blighted*, so as to create a relative balance between positive instances (blighted) and negative instanced (not blighted). The parcels thus removed from the data were selected at random.

The code and exploratory graphics used in the above analysis can be found at https://stuartbarnum.github.io/Detroit-Demolitions/Detriot_Draft_3.html

#### Construction of a tallies dataset
The predictive models that were eventually constructed in this project used data in the form of tallies (and other sums) from the data that had been processed as described above. All of the tallies were calculated using SQL-style joins on the basis of spatial relationships (`st_join`, again) or, in a few cases, identity of parcel numbers (using the `dplyr` function `inner_join`). After the joining, the results were then grouped and counted (using the `dplyr` package) as one does in SQL. In the applications of st_join, there were two types of tallies. In the first, I simply looked for incidents, such as citations for failure to maintain a property, occurring within a parcel. The other type of application of `st_join` involved looking for incidents that were within a certain distance from a parcel. In the latter type of case, I created a "buffer" around each parcel using the `sf` operation `st_buffer`, thus expanding each each of the parcels by a certain distance, depending on what seemed reasonable, given what I was attempting to count. With the parcels thus expanded, I applied `st_join`, now looking for incidents (and some other entities such as vacant lots) within the expanded parcels. At the end of all of this, the tallies dataset consisted of the parcel numbers for parcels in the balanced set of labels as described above, the value corresponding to blighted or not blighted, and 14 variables providing such tallies. The variables were, roughly, defined as follows.

```{r, echo = FALSE, message = FALSE}

library(readr)
library(knitr)
library(kableExtra)

variables <- read_csv("./data/variable_definitions.csv")
variables <- variables[complete.cases(variables),]

kable(variables, format = "html") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))

```

As indicated in the following summary statistics, the dataset consists of 4283 rows.

```{r, message = FALSE}

library(tidyverse)
complete_tally_set <- read_rds("calculated_tallies.rds")
summary(complete_tally_set)

```

Further details about this dataset (for example, the specific manner in which the crime-incident data was divided into violent crimes, property crimes, and nuisance crimes), including the code that was used in its construction from the raw data, can be found (again) at https://stuartbarnum.github.io/Detroit-Demolitions/Detriot_Draft_3.html. In the end, because our analysis is restricted to dismantle permits and demolitions after May of 2016, the variable `later_recorded_crime_incents` was not used in our models. I should also note that, as I built up the models, I did not treat all of the incidents in each of the datasets the same. For example, I assumed that complaints from, say, neighbors about failure to keep up a building would have a clear association with blight on the property for which the complaint was made, whereas complaints for failure of the city to fill-in potholes in a certain area may have a less clear connection with blight. Likewise, again, although it is difficult to make the distinction in a fully satisfactory way, I created separate variables for property crime, violent crime, and nuisance crime. And in fact, in many of my decision-tree models, violent crime had a weak but positive association with blight, whereas property crime had a weak but negative association with blight.

#### Models
For the modelling, I removed 15% of the data, to be reserved for a final check on our models. I then used the `createFolds` function from the `caret` package to partition the remaining data into 10 subsets, for k-fold cross validation. I constructed a number of models as I built up the tally dataset, beginning with both decision-tree models using `rpart` and logistic regression models using `glm`, on the the numbers of blight violations for each parcel and the total amount of fines for blight violations for each parcel. The initial decision-tree models on these two variables achieved an accuracy of roughly 60% percent, while the logistic regression models achieved an accuracy of 58%. Both of these model-types suggested that fine totals for each parcel provided a somewhat better predictor than number of violations for each parcel. Both of the model-types were notably poor predictors for positive instances (buildings that were blighted), as the following reconstruction, which includes confusion matrices for the k-fold cross-validation, indicates. (Note that blight corresponds to truth = 1.) 

```{r}
library(rpart)

#the parcel numbers for the examples that were not reserved for final testing
training_parcelnums <- read_rds("training_parcelnums.rds")

#separate the examples to be used for the training from the fifteen percent of 
#the examples to be withheld for final testing
train <- complete_tally_set %>% filter(parcelnum %in% training_parcelnums)
#test <- blight_violation_tallies %>% filter(parcelnum %in% testing_parcelnums)
train$blighted <- as.factor(train$blighted)

#partition the training set into ten subsets, while maintaining a ballance between 
#expamples labeled as blighted and examples not so labeled
set.seed(294)
k_folds <- caret::createFolds(train$blighted)

models <- 1:10 %>% map(~ rpart(blighted ~ total_fines_by_parcel + 
                                 number_of_violations_by_parcel,
                               data = train[-k_folds[[.x]],]))

predictions <- 1:10 %>% map(~ predict(models[[.x]], newdata = train[k_folds[[.x]],], 
                                      type = "class"))

accuracies <- 1:10 %>% 
  map(~ mean(unlist(predictions[[.x]] == train[k_folds[[.x]],]$blighted)))

#summary statistics over the models
mean(unlist(accuracies))
sd(unlist(accuracies))

#confusion matrices for predictions on the 10% of the training set that was not used to build the model
for (index in 1:10) {
  print(table(pred = (predictions[[index]]), 
              truth = train[k_folds[[index]],]$blighted))
}

#All of the rpart models in the k-fold cross-validation contained only one split---on 
#the total amount of fines related to blight on the parcel, as in the following.
models[[3]]

```

The rather weak but significant association between the irredeemable blight of a building and blight-related citations can also be seen in the following plot.

```{r}

library(tidyverse)

complete_tally_set %>% mutate(blighted = as.factor(blighted)) %>%
  filter(total_fines_by_parcel < 1200) %>% 
  ggplot(aes(x = total_fines_by_parcel, 
             color = blighted)) +
  geom_freqpoly(binwidth = 100) +
  labs(x = "total fines by parcel",
       y = "count per fine-amount interval of 100 dollars",
       title = "Distribution across Parcels of Total Fine Amounts") +
  theme(plot.title = element_text(hjust = 0.5)) + 
  coord_cartesian(xlim = c(0, 1200)) +
  scale_color_discrete(name = "parcel status",
                       breaks = c("1", "0"),
                       labels = c("blighted", "not blighted"))

```

As more variables were added to the tally dataset, I also applied random forest (using the `randomForest` package) and, finally, adaboost (using my own implementation in R) and support vector machines (using the `e1071` package). After the tally dataset was complete, I attempted to tune the models by changing the values of some of the parameters, such as the complexity parameter associated with the rpart function. In the end, most of the improvement in accuracy as I worked on this project was with the introduction of variables concerning blight at nearby parcels, especially in the case of the number of nearby vacant lots (other than the parcel for which prediction is taking place) and the number of nearby parcels, other than the parcel for which prediction is taking place, that became blighted prior to June of 2016. With the complexity parameter set at 0.003 (reduced from the default value of 0.01), the decision-tree models achieved an average accuracy, across the the ten models constructed in the k-fold cross validation, of 71%. Similar calculations of average accuracies for random forest and adaboost models yielded 0.73. The accuracy of 0.73 in the adaboost was achieved with the trees (the "weak" classifiers as per the adaboost algorithm) in the form of decision stumps---trees that contained at most one split, and in some cases no splits at all. Adaboost over trees containing more than one split resulted in accuracies at least somewhat less than this. The support vector machine models, after standardization of the numerical predictors, yielded an accuracy of 0.72.

I should emphasize that the counts of nearby blighted buildings did not include the building for which we were predicting, and was for the buildings that became blighted (as per our definition) before the time over which my models attempt to predict. Nevertheless, I tried running some of our approaches to model-building without using the variable for nearby blighted buildings. For the decision-tree models, this resulted in an average accuracy, over the 10 models constructed during k-fold cross validation, of 0.70. 

Most of the model constructions described above can be found at https://stuartbarnum.github.io/Detroit-Demolitions/Detroit_models_from_tallies.html.

#### Final thoughts and analysis
Given the relative concreteness and understandability of decision trees, together with the fact that these models were only somewhat less effective as predictors than the more advanced models, I suspect that the the decision trees may provide the best of the models for at least some uses. With this in mind, it is especially notable that a remarkably simple model may provide a useful predictor. First of all, consider the analysis that provided the most-accurate decision trees.

```{r}

#separate the examples to be used for the training from the twenty percent of the 
#examples to be withheld for final testing (remembering that we will do 10-fold 
#cross validation over the training set)

#test <- blight_violation_tallies %>% filter(parcelnum %in% testing_parcelnums)
complete_tally_set$blighted <- as.factor(complete_tally_set$blighted)

#partition the training set into ten subsets, while maintaining a ballance 
#between expamples labeled as blighted and examples not so labeled
set.seed(451)
k_folds <- caret::createFolds(complete_tally_set$blighted)

complete_formula <- blighted ~ total_fines_by_parcel + number_of_violations_by_parcel + 
    improve_issues_tallies + earlier_property_crime_incidents + 
    earlier_violent_crime_incidents + total_acre + council_di + 
    frontage + num_vacant_parcels + num_nearby_blighted_parcels + 
    num_violations_nearby_parcels + earlier_nuiscance_offences

models <- 1:10 %>% map(~ rpart(complete_formula, 
                               data = complete_tally_set[-k_folds[[.x]],],
                               method = "class", control = rpart.control(cp = 0.003)))

predictions <- 1:10 %>% map(~ predict(models[[.x]], 
                                      newdata = complete_tally_set[k_folds[[.x]],], 
                                      type = "class"))

accuracies <- 1:10 %>% 
  map(~ mean(as.numeric(predictions[[.x]] == complete_tally_set[k_folds[[.x]],]$blighted)))

#summary statistics over the models
mean(as.numeric(accuracies))
sd(as.numeric(accuracies))

#confusion matrices for predictions on the 10% of the training set that was not used 
#to build the model
for (index in 1:10) {
  print(table(pred = predictions[[index]], 
              truth = complete_tally_set[k_folds[[index]],]$blighted))
}

models[[5]]

```

I should note that, although I have only displayed one of the models that were generated in this implementation of k-fold cross validation, the other models are similarly complex. Furthermore, experimentation with the complexity parameter in the rpart function suggests that, despite the relative complexity of these models, they may not be overfit---adjusting the complexity parameter even somewhat lower produces a simpler model that is somewhat less accurate (yields a somewhat lesser average accuracy under k-fold cross validation). (On the other hand, adjusting the complexity parameter lower results in a more complex model that predicts less well: thus perhaps a model that *is* overfit). However, if we adjust the complexity parameter to the `rpart` default value of 0.01, we obtain a strikingly simple model that is only somewhat less accurate than the rather complex models constructed in the above. 

```{r}

models <- 1:10 %>% map(~ rpart(complete_formula, 
                               data = complete_tally_set[-k_folds[[.x]],],
                               method = "class", control = rpart.control(cp = 0.01)))

predictions <- 1:10 %>% map(~ predict(models[[.x]], 
                                      newdata = complete_tally_set[k_folds[[.x]],], 
                                      type = "class"))

accuracies <- 1:10 %>% 
  map(~ mean(as.numeric(predictions[[.x]] == complete_tally_set[k_folds[[.x]],]$blighted)))

#summary statistics over the models
mean(as.numeric(accuracies))
sd(as.numeric(accuracies))

#confusion matrices for predictions on the 10% of the training set that was not used 
#to build the model
for (index in 1:10) {
  print(table(pred = predictions[[index]], 
              truth = complete_tally_set[k_folds[[index]],]$blighted))
}

models[[3]]

```

I should note that all of the models generated in the above k-fold cross-valiation process have the same form as does the above decision-tree model (with somewhat different break points, such as with 52.5 instead of 50.5 at node 4).  

As is well-known about the city of Detroit, many of of the vacant parcels once contained buildings, which were torn down due to bight. Given the suggestion that the number of nearby blighted parcels and the number of nearby vacant parcels are especially effective as predictors, we see that past and more recent nearby blight may provide the best predictors of the blightedness of any given building. It may thus be interesting, finally, to consider a plot of joint distribution of these two predictors across the parcels in our balanced dataset, and then a plot of the concentration of blightedness as a function of the two predictors.

```{r}

complete_tally_set %>% filter(num_nearby_blighted_parcels < 20 & num_vacant_parcels < 200) %>%
  ggplot() + stat_bin_2d(aes(x = num_nearby_blighted_parcels,
                             y = num_vacant_parcels),
                         bins = 15) +
  ggtitle("Distribution of num_nearby_blighted_parcels and num_vacant_parcels
          (within the balanced dataset)") +
  theme(plot.title = element_text(hjust = 0.5))

complete_tally_set %>% mutate(blighted = as.numeric(blighted) - 1) %>%
  filter(num_nearby_blighted_parcels < 20 & num_vacant_parcels < 200) %>%
  ggplot() + stat_summary_2d(aes(x = num_nearby_blighted_parcels,
                                 y = num_vacant_parcels,
                                 z = blighted), 
                             bins = 15) +
  ggtitle("Blightedness as a function of num_nearby_blighted_parcels \nand num_vacant_parcels") +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_fill_continuous(name = "mean\nblightedness")


```
